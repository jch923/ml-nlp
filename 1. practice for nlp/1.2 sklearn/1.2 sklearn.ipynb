{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 사이킷 런(Sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.1'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사이킷런 참조, 버전 확인\n",
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기\n",
    "#### 붓꽃 데이터를 통해 머신러닝의 기초를 배워보고자 한다.\n",
    "#### 불러온 데이터를 분석하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 붓꽃 데이터 불러오기\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_dataset key: dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "# 변수에 데이터 할당\n",
    "iris_dataset = load_iris()\n",
    "# 데이터가 어떤 값으로 구성돼 있는지 출력\n",
    "print(\"iris_dataset key: {}\".format(iris_dataset.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "shape of data: (150, 4)\n"
     ]
    }
   ],
   "source": [
    "# data(실제 데이터)키 값 확인\n",
    "print(iris_dataset['data'])\n",
    "\n",
    "# 각 데이터마다 4개의 특징값을 가지고 있다.\n",
    "\n",
    "# 데이터의 형태 확인\n",
    "print(\"shape of data: {}\". format(iris_dataset['data'].shape))\n",
    "\n",
    "# 150개의 데이터가 각각 4개의 특징값을 가지고 있는 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# feature_names(4개의 특징값) 키 값 확인\n",
    "print(iris_dataset['feature_names'])\n",
    "\n",
    "# 4개의 특징값은 각각 각 데이터, 즉 꽃의 꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# target(라벨 값)값과 target_names(라벨 값의 의미)값 출력\n",
    "print(iris_dataset['target'])\n",
    "print(iris_dataset['target_names'])\n",
    "\n",
    "# 각 데이터는 각자의 라벨값을 가지고 있으며 총 3개로 나뉨을 확인할 수 있음\n",
    "# 각 라벨은 setosa, versicolor, virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "# DESCR값 출력\n",
    "print(iris_dataset['DESCR'])\n",
    "\n",
    "# 전체 요약정보(description)를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1. 싸이킷-런 데이터 분리\n",
    "#### 사이킷 런을 이용하면 학습 데이터를 대상으로 학습 데이터와 평가 데이터로 쉽게 나눌 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = iris_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trin_test_split 함수는 학습 데이터와 라벨을 넣으면 정의한 비율로 데이터를 나눈다\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터와 평가데이터로 비율을 맞춰서 나눠준다\n",
    "train_input, test_input, train_label, test_label = train_test_split(iris_dataset['data'],\n",
    "                                                                    target,\n",
    "                                                                    test_size = 0.25,\n",
    "                                                                    random_state=42)\n",
    "\n",
    "# test_size: 전체 학습 데이터중 일정 비율을 나눠준다는 의미(ex) 0.25 = 25%)\n",
    "# random_state: 함수가 데이터를 나눌 때 무작위로 데이터를 선택해서 나누는데, 이때 무작위로 선택되는 것을 제어할 수 있는 값\n",
    "#           함수를 여러번 사용하더라도 이 값을 똑같이 설정할 경우 동일한 데이터를 선택해서 분리할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_input: (112, 4)\n",
      "shape of test_input: (38, 4)\n",
      "shape of train_label: (112,)\n",
      "shape of test_label: (38,)\n"
     ]
    }
   ],
   "source": [
    "# 각 변수의 형태를 확인\n",
    "print(\"shape of train_input: {}\".format(train_input.shape))\n",
    "print(\"shape of test_input: {}\".format(test_input.shape))\n",
    "print(\"shape of train_label: {}\".format(train_label.shape))\n",
    "print(\"shape of test_label: {}\".format(test_label.shape))\n",
    "\n",
    "# train_input, train_label(학습 데이터)은 총 112개\n",
    "# test_input, test_label(평가 데이터)은 총 38개\n",
    "# 학습 데이터와 평가데이터가 따로 존재하지 않는 경우에는 이처럼 학습 데이터의 일부분을 평가 데이터로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2. 싸이킷-런 지도 학습\n",
    "### 사이킷 런을 통해 지도학습 모델을 만드는 방법을 알아본다.\n",
    "### 지도학습이란 각 데이터에 대해 정답이 있는 경우 각 데이터의 정답을 예측할 수 있게 학습시키는 과정이다.\n",
    "### 여기서는 지도 학습 모델 중 하나인 k-최근접 이웃 분류기를 사용한다.\n",
    "### k-최근접 이웃 분류기는 예측하고자 하는 데이터에 대해 가장 가까운 거리에 있는 데이터의 라벨과 같다고 예측하는 방법이다.\n",
    "### 여기서 k 값은 예측하고자 하는 데이터와 가까운 몇개의 데이터를 참고할 것인지를 의미한다.\n",
    "### k-최근접 이웃 분류기으 특징은 다음과 같다.\n",
    "#### - 데이터에 대한 가정이 없어 단순하다.\n",
    "#### - 다목적 분류와 회귀에 좋다.\n",
    "#### - 높은 메모리를 요구한다.\n",
    "#### - k값이 커지면 계산이 늦어질 수 있다.\n",
    "#### - 관련 없는 기능의 데이터의 규모에 민감하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류기 객체를 불러와 변수에 할당한다.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# 가까운 데이터 하나를 참고하는 분류기 생성\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "\n",
    "# n_neighbors: 위에서 설명한 k값(가까운 데이터 중 몇개의 데이터를 참고할 것인지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분류기를 학습 데이터에 적용\n",
    "knn.fit(train_input, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4개의 특징값을 넘파이 배열로 생성\n",
    "import numpy as np\n",
    "# 꽃받침 길이와 너비가 각각 6.1, 2.8 이고 꽃잎의 길이와 너비가 각각 4.7, 1.2인 데이터 생성\n",
    "new_input = np.array([[6.1, 2.8, 4.7, 1.2]])\n",
    "\n",
    "# 여기서 리스트안에 또하나의 리스트가 포함된 방식으로 만들지 않으면 에러가 난다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이 값을 대상으로 분류기 모델의 predict 함수를 사용해 결과를 예측 \n",
    "knn.predict(new_input)\n",
    "\n",
    "# 결과는 1(Versicolor)로 예측한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "# 임의로 만든 데이터값을 제대로 예측한 것인지 확인해야한다.\n",
    "# 평가 데이터를 사용해 모델의 성능을 측정해야한다.\n",
    "\n",
    "# 평가 데이터에 대해 예측하고 결과값을 변수에 저장\n",
    "predict_label = knn.predict(test_input)\n",
    "# 예측한 결괏값 출력\n",
    "print(predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 1.00\n"
     ]
    }
   ],
   "source": [
    "# 예측한 결과값과 실제 결과값의 정확도 구분\n",
    "print('test accuracy {:.2f}'.format(np.mean(predict_label == test_label)))\n",
    "\n",
    "# 1.00(100%)로 좋은 성능을 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3. 싸이킷-런 비지도 학습\n",
    "### 비지도 학습이란 데이터에 대한 정답, 즉 라벨을 사용하지 않고 만들 수 있느 ㄴ모델이다.\n",
    "### 모델을 통해 문제를 해결하고 싶은데 데이터에 대한 정답이 없는 경우에 적용하기에 적합한 모델이다.\n",
    "### 여기서는 k- 평균 군집화 모델을 사용한다.\n",
    "### 군집이란 데이터를 특성에 따라 여러 집단으로 나누는 방법이다.\n",
    "### 붓꽃 데이터의 경우 3개의 군집으로 나누는 방법을 사용해야 한다.\n",
    "### k-평균 군집은 k개만큼의 중심을 임의로 설정하고, 모든 데이터를 가장 가까운 중심에 할당하여 같은 중심에 할당된 데이터들을 하나의 군집으로 판단한다.\n",
    "### 각 군집 내 데이터들을 가지고 군집의 중심을 새로 구해서 업데이트 한다.\n",
    "### 이 과정을 데이터에 변화가 없을 때까지 반복한다.\n",
    "### 이과 정이 끝나고 난 뒤 중심에 따라 군집이 나뉜다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 군집화 모듈에서 KMeans를 불러온 후 k-평균 군집화 모델을 만든다.\n",
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters=3)\n",
    "\n",
    "# n_cluster: 군집의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 군집화 모델에 데이터 적용\n",
    "k_means.fit(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 1, 1, 0, 0, 2, 0, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 2, 1,\n",
       "       1, 0, 2, 1, 0, 1, 1, 0, 0, 2, 0, 2, 2, 0, 1, 1, 0, 2, 1, 1, 1, 0,\n",
       "       2, 1, 2, 2, 1, 0, 0, 0, 2, 2, 1, 2, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       2, 2, 1, 0, 2, 2, 1, 2, 1, 2, 2, 2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 2])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라벨값을 넣지 않았기 때문에 k-평균 군집화 모델의 라벨 속성으 ㄹ확인한다\n",
    "k_means.labels_\n",
    "\n",
    "# 여기서 숫자는 앞과는 달리 붓꽃의 라벨을 의미하는 것이 아니라 각 군집을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cluster: [2 1 1 1 2 1 1 1 1 1 2 1 1 1 2 2 2 1 1 1 1 1 2 1 1 1 1 2 1 1 1 2 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 2 1 2 1]\n",
      "1 cluster: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "2 cluster: [2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 1 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# 붓꽃 종의 분포를 확인\n",
    "print(\"0 cluster:\", train_label[k_means.labels_ == 0])\n",
    "print(\"1 cluster:\", train_label[k_means.labels_ == 1])\n",
    "print(\"2 cluster:\", train_label[k_means.labels_ == 2])\n",
    "\n",
    "# 0번째 군집은 0, 1번째 군집은 1, 2번째 군집은 2가 주로 분포 되어있음\n",
    "# 이는 군집화 할 때마다 바뀔 수 있다.\n",
    "# 만약 0번째 군집이 1이 주로 분포 되어있었다면 0번째 군집은 Versicolour이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 새로운 데이터를 만듦\n",
    "import numpy as np\n",
    "new_input  = np.array([[6.1, 2.8, 4.7, 1.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 데이터로 예측을 진행\n",
    "prediction = k_means.predict(new_input)\n",
    "print(prediction)\n",
    "\n",
    "# 결과는 앞과 같이 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 0 0 1 0 2 0 0 2 1 1 1 1 0 2 0 0 2 1 0 1 2 2 2 2 2 1 1 1 1 0 1 1 0 0\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "# 평가 데이터를 예측 \n",
    "predict_cluster = k_means.predict(test_input)\n",
    "print(predict_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# 각 데이터가 속한 군집의 순서를 실제 붓꽃의 라벨로 바꿔줌\n",
    "np_arr = np.array(predict_cluster)\n",
    "# 이를 변경할 때는 임시 저장을 위해 군집의 순서값 3,4,5를 넘파이 배열로 만들어준다.\n",
    "np_arr[np_arr==0], np_arr[np_arr==1], np_arr[np_arr==2] = 3, 4, 5\n",
    "# 여기서 실제 라벨은 그때그때 군집 생성에 따라 바꿔줘야 한다.\n",
    "np_arr[np_arr==3] = 1\n",
    "np_arr[np_arr==4] = 0\n",
    "np_arr[np_arr==5] = 2\n",
    "predict_label = np_arr.tolist()\n",
    "print(predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.95\n"
     ]
    }
   ],
   "source": [
    "# 정확도 확인\n",
    "print('test accuracy {:.2f}'.format(np.mean(predict_label == test_label)))\n",
    "\n",
    "# 100%는 아니나 라벨을 사용하지 않고도 95%로 거의 정확함을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4. 싸이킷-런 특징 추출\n",
    "### 수치형 데이터가 아닌 경우 특징 추출 모듈을 사용한다.\n",
    "### 자연어 처리에서 특징 추출이란 텍스트 데이터에서 단어나 문장들을 어떤 특징 값으로 바꿔주는 것을 의미한다.\n",
    "### 기존에 문자로 구성돼 있던 데이터를 모델에 적용할 수 있도록 특징을 뽑아 어떤 값으로 바꿔서 수치화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________\n",
    "### CountVectorizer\n",
    "#### CountVectorizer는 텍스트 데이터에서 횟수를 기준으로 특징을 추출하는 방법이다.\n",
    "#### 여기서 어떤 단위의 횟수를 셀 것인지는 선택 상항이다.\n",
    "#### 보통은 텍스트에서 단어를 기준으로 횟수를 측정하는데, 문장을 입력받아 단어의 횟수를 측정한 뒤 벡터로 만든다.\n",
    "#### 횟수를 사용해서 벡터를 만들기 때문에 직관적이고 간단해서 여러 상황에서 사용할 수 있다는 장점이 있다.\n",
    "#### 단순히 횟수만을 특징으로 잡기 때문에 큰 의미가 없지만 자주 사용되는 단어들, 예를들면 조사 혹은 지시대며사가 높은 특징 값을 가지기 때문에 유의미하게 사용하기 어려울 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷 런의 모듈 중 CountVectorizer를 불러온다\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 데이터를 임의로 생성한다.\n",
    "text_data = ['나는 배가 고프다', '내일 점심 뭐먹지', '내일 공부 해야겠다', '점심 먹고 공부 해야지']\n",
    "# CountVectorizer 객체를 생성한다.\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}\n"
     ]
    }
   ],
   "source": [
    "# fit을 사용하여 생성한 객체에 데이터를 적용하여 자동으로 단어 사전을 생성한다.\n",
    "count_vectorizer.fit(text_data)\n",
    "# 데이터 적용 후 단어사전 출력\n",
    "print(count_vectorizer.vocabulary_)\n",
    "\n",
    "# 각 단어에 대해 숫자들이 사전 형태로 구성돼 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 정의한 텍스트 데이터중 하나만 벡터로 만든다.\n",
    "sentence = [text_data[0]] # ['나는 배가 고프다']\n",
    "# 결과 출력\n",
    "print(count_vectorizer.transform(sentence).toarray())\n",
    "\n",
    "# 각 단어가 1번씩 나왔으므로 나온 단어들의 위치가 1이 됨을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "#### 특정한 값을 사용해서 텍스트 데이터의 특징을 추출하는 방법\n",
    "#### TF란 특정 단어가 하나의 데이터 안에서 등장 하는 횟수를 의미한다.\n",
    "#### IDF는 특정 단어가 다른데이터에 등장하지 않는 횟수를 의미한다.(자주 등장하지 않을수록 값이 커진다)\n",
    "#### 이 두값을 곱해서 사용하므로 해당 문서에서 자주 등장하지만 다른 문서에는 많이 없는 단어일 수록 높은 값을 가진다.\n",
    "#### 조사나 지시대명사처럼 자주 등장하는 단어는 TF값은 크지만 IDF값은 작아지므로 CountVectorizer가 가진 문제점을 해결할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷런의 특징 추출 모듈에서 TfidfVectorizer를 불러온다.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 텍스트 데이터 생성\n",
    "text_data = ['나는 배가 고프다', '내일 점심 뭐먹지', '내일 공부 해야겠다', '점심 먹고 공부 해야지']\n",
    "# TfidfVectorizer 객체 생성\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}\n",
      "[[0.         0.43779123 0.         0.         0.55528266 0.\n",
      "  0.         0.43779123 0.         0.55528266]]\n"
     ]
    }
   ],
   "source": [
    "# fit을 통해 단어 사전을 만들고 확인해본다.\n",
    "tfidf_vectorizer.fit(text_data)\n",
    "print(tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "# 한 문장만 객체에 적용해 벡터값을 확인해본다.\n",
    "sentence = [text_data[3]] # ['점심 먹고 공부 해야지']\n",
    "print(tfidf_vectorizer.transform(sentence).toarray())\n",
    "\n",
    "# 사용되지 않은 단어는 0이나온다.\n",
    "# 사용된 단어중 좀더 높은 값을 가지는 단어들은 해당 문장에서만 쓰였고\n",
    "# 상대적으로 낮은 값을 가지는 단어들은 여러 문장에서 쓰인 단어들이었음을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
