{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator\n",
    "_________________________________________________________________\n",
    "## 에스티메이터는 딥러닝 모델을 최대한 빠르게 실험해보고 다양한 모델을 적용할 수 있게 해줘서 모델 구현에 집중할 수 있는 환경을 제공한다.\n",
    "## 에스티메이터는 고수준 API를 통해 이러한 환경을 제공한다.\n",
    "## 모델 구현 외의 학습(Train), 검증(Evaluate), 예측(Predict)에 필요한 부가적인 구현들은 함수를 통해 제공한다.\n",
    "## 모든 검증과 평가가 끝난 모델은 배포(Export)모델을 통해 구현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________________________________________\n",
    "### 에스티메이터에서 제공하는 기본적인 기능\n",
    "#### - 학습(Train) : 정의한 모델 파라미터에 대해 학습한다.\n",
    "#### - 평가(Evaluate): 학습한 모델에 대한 성능을 측정한다.\n",
    "#### - 예측(Predict): 모델을 통해 입력값에 대한 예측값을 받는다.\n",
    "#### - 배포(Export): 사용할 모델을 바이너리 파일로 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________\n",
    "#### 에스티메이터를 구현하기 위해서는 기본적으로 model_fn()과 train_input_fn()을 구현해야한다.\n",
    "#### model함수의 인자는 총 다섯개이며 다음과같다.\n",
    "##### - features: 모델에 적용되는 입력값, 학습, 검증, 예측 과정 모두에 사용된다.\n",
    "##### - labels: 모델의 정답 라벨값을 의미한다., 예측 과정에서는 라벨이 존재하지 않기 떄문에 에스티메이터에서 자동으로 이 값이 들어오지 않는다\n",
    "##### - mode: 현재 모델 함수가 실행된 모드(학습, 검증, 예측)를 의미한다.\n",
    "##### - params: 모델에 적용될 부가적인 하이퍼파라미터 값을 의미한다.\n",
    "##### - config: 모델에 적용할 설정값을 의미한다.\n",
    "#### 이중 features와 labels는 필수적으로 인자로 받도록 설정, 나머지는 필요에따라 선택적으로 받을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터는 이전 장에서 사용했던 텍스트 데이터를 그대로 사용\n",
    "# 전처리 과정 또한 동일하게 적용\n",
    "samples = ['너 오늘 이뻐 보인다', \n",
    "           '나는 오늘 기분이 더러워', \n",
    "           '끝내주는데, 좋은 일이 있나봐', \n",
    "           '나 좋은 일이 생겼어', \n",
    "           '아 오늘 진짜 짜증나', \n",
    "           '환상적인데, 정말 좋은거 같아']\n",
    "\n",
    "labels = [[1], [0], [1], [1], [0], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 함수에 필요한 에폭 값\n",
    "EPOCH = 100\n",
    "\n",
    "# 데이터를 에스티메이터에 전달하기 위한 데이터 입력 함수\n",
    "# tf.data의 반복, 셔플 기능을 통해 입력 함수를 구현\n",
    "# 정의한 에폭 크기만큼 데이터를 반복시켜서 학습하게 하고, 셔플기능을 통해 모델이 학습을 잘 할 수 있게 한다.\n",
    "# 함수 반환값은 이터레이터의 get_next함수를 사용하여 입력값과 라벨값을 반환한다.\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
    "    dataset = dataset.repeat(EPOCH)\n",
    "    dataset = dataset.batch(1)\n",
    "    dataset = dataset.shuffle(len(sequences))\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 함수에 필요한 단어 개수와 임베딩 벡터 크기의 상숫값\n",
    "VOCAB_SIZE = len(word_index) +1\n",
    "EMB_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 모델을 구현한 모델함수\n",
    "# 이를 Estimator 객체에 적용하면 된다.\n",
    "# 이를 통해 생성한 객체는 학습, 검증, 예측을 사용할 수 있다.\n",
    "\n",
    "# 인자로 입력값과 라벨, 그리고 현재 모델 함수가 실행되는 상태인 모드값을 받는다.\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # TRAIN, EVALUATE, PREDICT상태 중 해당하는 상태의 상숫값이 True가 된다.\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    # 입력값을 임베딩 벡터 형태로 만들어준다\n",
    "    embed_input = tf.keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features)\n",
    "    # 텐서플로의 reduce_mean 함수를 통해 평균을 구해서 하나의 입력 벡터로 만들어준다.\n",
    "    embed_input = tf.reduce_mean(embed_input, axis=-1)\n",
    "    \n",
    "    # 이 입력값을 Dense를 사용해 은닉층을 거쳐 출력값을 만든다.\n",
    "    hidden_layer = tf.keras.layers.Dense(128, activation=tf.nn.relu)(embed_input)\n",
    "    output_layer = tf.keras.layers.Dense(1)(hidden_layer)\n",
    "    # 시그모이드 함수를 적용해 0과 1사이의 값으로 만든다\n",
    "    # 이것이 모델의 최종 출력값이다\n",
    "    output = tf.nn.sigmoid(output_layer)\n",
    "    \n",
    "    # 학습 상태일 때 모델을 학습할 수 있도록 손실(loss)값과 옵티마이저(optimizer)를 설정해야한다.\n",
    "    \n",
    "    # 손실 함수는 평균 제곱 오차를 이용\n",
    "    loss = tf.losses.mean_squared_error(output, labels)\n",
    "\n",
    "    # 옵티마이저는 아담 옵티마이저 사용\n",
    "    # 두 값을 학습 모드인경우 EstimatorSpec의 인자갑으로 반환한다.\n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  train_op=train_op,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './data_out/checkpoint/dnn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# 실제 학습\n",
    "\n",
    "# 학습 과정을 체크포인트로 저장할 폴더를 만들어준다.\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "\n",
    "# os라이브러리를 사용해서 폴더의 유무를 확인한 후 존재하지 않는다면 폴더를 만들어준다.\n",
    "import os\n",
    "\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "# 앞서 정의한 모델 함수와 체크포인트를 저장할 폴더를 인자로 넣어준다.\n",
    "estimator = tf.estimator.Estimator(model_fn = model_fn, model_dir = DATA_OUT_PATH + 'checkpoint/dnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jch\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From <ipython-input-4-1dae89998461>:8: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From C:\\Users\\jch\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:118: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\jch\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./data_out/checkpoint/dnn\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 0.24992156, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1233.45\n",
      "INFO:tensorflow:loss = 0.1711032, step = 101 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 2497.71\n",
      "INFO:tensorflow:loss = 0.0171644, step = 201 (0.040 sec)\n",
      "INFO:tensorflow:global_step/sec: 2561.78\n",
      "INFO:tensorflow:loss = 0.0108717615, step = 301 (0.039 sec)\n",
      "INFO:tensorflow:global_step/sec: 2561.77\n",
      "INFO:tensorflow:loss = 0.0008987792, step = 401 (0.039 sec)\n",
      "INFO:tensorflow:global_step/sec: 2561.75\n",
      "INFO:tensorflow:loss = 0.00198549, step = 501 (0.040 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 600...\n",
      "INFO:tensorflow:Saving checkpoints for 600 into ./data_out/checkpoint/dnn\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 600...\n",
      "INFO:tensorflow:Loss for final step: 0.000915157.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x1b2b3dde460>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.train(train_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
