{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.6 재현 신경망(Recurrent Neural Network) 분류 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jch\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터 파일 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "\n",
    "TRAIN_INPUT_DATA = 'train_input.npy'\n",
    "TRAIN_LABEL_DATA = 'train_label.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 넘파이 형태로 넘어온 파일을 오픈하여 학습 데이터를 구성한다.\n",
    "input_data = np.load(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'rb'))\n",
    "label_data = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'rb'))\n",
    "prepro_configs = None\n",
    "\n",
    "# 단어 사전은 json으로 저장했으므로 json으로 불러온다.\n",
    "with open(DATA_IN_PATH + DATA_CONFIGS, 'r') as f:\n",
    "    prepro_configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.1\n",
    "RANDOM_SEED = 13371447\n",
    "\n",
    "# 학습 데이터와 평가 데이터로 나눈다.\n",
    "train_input, test_input, train_label, test_label = train_test_split(input_data, label_data, \n",
    "                                                                    test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# 데이터 입력 함수 구현\n",
    "\n",
    "# 매핑 함수\n",
    "def mapping_fn(X, Y=None):\n",
    "    inputs, labels = {'x': X}, Y\n",
    "    return inputs, labels\n",
    "\n",
    "# 학습 데이터 입력 함수\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_input, train_label))\n",
    "    dataset = dataset.shuffle(buffer_size=50000)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.repeat(count=NUM_EPOCHS)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "# 평가 데이터 입력 함수\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((test_input, test_label))\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    dataset = dataset.batch(BATCH_SIZE * 2)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 하이퍼파라미터 정의\n",
    "\n",
    "# 단어 사전에 대한 크기\n",
    "VOCAB_SIZE = prepro_configs['vocab_size']+1\n",
    "# 각 변수의 차원과 학습률\n",
    "WORD_EMBEDDING_DIM = 100\n",
    "HIDDEN_STATE_DIM = 150\n",
    "DENSE_FEATURE_DIM = 150\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74065 74066\n"
     ]
    }
   ],
   "source": [
    "print(len(prepro_configs['vocab']), VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 함수\n",
    "def model_fn(features, labels, mode):\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    # 임베딩 층\n",
    "    \n",
    "    # 입력값을 임베딩 벡터 형태로 만든다.\n",
    "    embedding_layer = tf.keras.layers.Embedding(\n",
    "                    VOCAB_SIZE,\n",
    "                    WORD_EMBEDDING_DIM)(features['x'])\n",
    "    \n",
    "    # 임베딩 벡터에 20%의 dropout 적용\n",
    "    embedding_layer = tf.keras.layers.Dropout(0.2)(embedding_layer)\n",
    "    \n",
    "    # 순환 신경망 구현\n",
    "    \n",
    "    # 심층 순환 신경망 모델은 LSTM모델을 통해 구현\n",
    "    \n",
    "    # 순환 신경망 구현을 위해 RNNCell이란 객체를 활용한다.\n",
    "    # RNNCell은 순환 신경망 객체라고 보면 된다.\n",
    "    rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [HIDDEN_STATE_DIM, HIDDEN_STATE_DIM]]\n",
    "    # 여러 LSTMCell을 쌓게되면 하나의 MultiRNN으로 래핑해야한다.\n",
    "    multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
    "\n",
    "    # 네트워크와 임베딩 벡터와 연산하기 위해 dynamic_rnn함수를 선언한다.\n",
    "    # RNNCell객체는 시퀀스 한 스텝에 대한 연산만 가능하다. 따라서 여러 스텝을 연산하기 위해 for문을 활용해야한다.\n",
    "    # dynamic_rnn함수는 for문 없이 자동으로 순환 신경만을 만들어 주는 역할을 한다.\n",
    "    # 입력 인자로는 cell에 순환 신경망 객체, inputs에는 입력값, dtype에는 출력값의 데이터 타입을 넣는다.\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,\n",
    "                                       inputs=embedding_layer,\n",
    "                                       dtype=tf.float32)\n",
    "    \n",
    "    # 출력값에도 20%의 dropout을 적용한다.\n",
    "    outputs = tf.keras.layers.Dropout(0.2)(outputs)\n",
    "    # Dense에 적용시키는 입력값은 LSTM 신경망의 마지막 출력값을 넣어준다.\n",
    "    hidden_layer = tf.keras.layers.Dense(DENSE_FEATURE_DIM, activation=tf.nn.tanh)(outputs[:,-1,:])\n",
    "    # 이 값 또한 20%의 droptout을 적용한다.\n",
    "    hidden_layer = tf.keras.layers.Dropout(0.2)(hidden_layer)\n",
    "    \n",
    "    # 최종적으로 1dim의 output만 할 수 있도록 dense layer를 활용해 차원 변환을 한다.\n",
    "    logits = tf.keras.layers.Dense(1)(hidden_layer)\n",
    "    logits = tf.squeeze(logits, axis=-1)\n",
    "    \n",
    "    # 시그모이드 함수를 적용해 0과 1사이의 값으로 바꾼다.\n",
    "    sigmoid_logits = tf.nn.sigmoid(logits)\n",
    "    \n",
    "    # 모델 예측을 위한 구현\n",
    "    # 모델 성능을 어느 정도 평가하고 나면 캐글에 평가할 데이터를 가지고 예측해야 한다.\n",
    "    if PREDICT:\n",
    "        # 시그모이드 함수가 적용된 모델 출력값을 dict 객체로 표현해서 에스티메이터에서 반환할 예측값을 만든다.\n",
    "        predictions = {'sentiment': sigmoid_logits}\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  predictions=predictions)\n",
    "    \n",
    "    # 모델에서 구현한 값과 정답 라벨을 가지고 손실 값을 구한다.\n",
    "    loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "    \n",
    "    # 모델 평가를 위한 구현\n",
    "    if EVAL:\n",
    "        # 감정 예측값이 정답 라벨과 얼마나 일치하는지 정확도를 본다.\n",
    "        # 정확도는 tf.metrics.accuracy를 통해 나타낼 수 있다.\n",
    "        accuracy = tf.metrics.accuracy(labels, tf.round(sigmoid_logits))\n",
    "        eval_metric_ops = {'acc': accuracy}\n",
    "        \n",
    "        # accuracy값을 tf.estimator.EstimatorSpec의 eval_metric_ops인자로 입력하면 평가 결괏값을 확인할 수 있다.\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "    \n",
    "    # 모델 학습을 위한 구현\n",
    "    if TRAIN:\n",
    "        # 현재 학습 글로벌 스텝 얻기\n",
    "        global_step = tf.train.get_global_step()\n",
    "        # AdamOptimizer 객체 생성\n",
    "        # 예측 손실값을 구하고 나면 파라미터 최적화를 하고자 경사도 하강법을 진행한다.\n",
    "        # 여기서는 경사도 하강법으로 아담 옵티마이저를 활용한다.\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step)\n",
    "        \n",
    "        # 직접 모델 함수를 구현하게 되면 tf.estimator.EstimatorSpec 객체를 생성해서 반환하게 한다.\n",
    "        # 이 객체는 현재 함수가 어느 모드에서 실행되고 있는지 확인한다. 그리고 모드에 따라 입력 인자가 다르다.\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  train_op=train_op,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './data_out/checkpoint/rnn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "# tf/estimator.Estimator객체 생성\n",
    "# 인자는 작성한 모델함수를 넣는다.\n",
    "est = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                             model_dir=DATA_OUT_PATH + 'checkpoint/rnn') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-37-58087bf0bfb1>:18: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From C:\\Users\\jch\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:118: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-40-30d7c818af9f>:23: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-40-30d7c818af9f>:25: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-40-30d7c818af9f>:31: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\jch\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:962: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\jch\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:970: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./data_out/checkpoint/rnn\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 0.6934498, step = 1\n",
      "INFO:tensorflow:global_step/sec: 6.35514\n",
      "INFO:tensorflow:loss = 0.7002388, step = 101 (15.736 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.62132\n",
      "INFO:tensorflow:loss = 0.6976713, step = 201 (15.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.63936\n",
      "INFO:tensorflow:loss = 0.6949645, step = 301 (15.063 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.63275\n",
      "INFO:tensorflow:loss = 0.6611054, step = 401 (15.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.68512\n",
      "INFO:tensorflow:loss = 0.7025956, step = 501 (14.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.72925\n",
      "INFO:tensorflow:loss = 0.6817167, step = 601 (14.860 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.79331\n",
      "INFO:tensorflow:loss = 0.6923512, step = 701 (14.720 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.69812\n",
      "INFO:tensorflow:loss = 0.690406, step = 801 (14.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.81555\n",
      "INFO:tensorflow:loss = 0.6853168, step = 901 (14.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.71342\n",
      "INFO:tensorflow:loss = 0.725212, step = 1001 (14.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.7827\n",
      "INFO:tensorflow:loss = 0.6963284, step = 1101 (14.742 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.82859\n",
      "INFO:tensorflow:loss = 0.6680273, step = 1201 (14.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.63936\n",
      "INFO:tensorflow:loss = 0.6801224, step = 1301 (15.063 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.78823\n",
      "INFO:tensorflow:loss = 0.73343074, step = 1401 (14.730 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.7406\n",
      "INFO:tensorflow:loss = 0.6784035, step = 1501 (14.835 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.75655\n",
      "INFO:tensorflow:loss = 0.63074344, step = 1601 (14.801 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.73333\n",
      "INFO:tensorflow:loss = 0.6633365, step = 1701 (14.854 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.83701\n",
      "INFO:tensorflow:loss = 0.7007924, step = 1801 (14.622 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.72246\n",
      "INFO:tensorflow:loss = 0.6505266, step = 1901 (14.877 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.9103\n",
      "INFO:tensorflow:loss = 0.6951121, step = 2001 (14.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.51638\n",
      "INFO:tensorflow:loss = 0.7425179, step = 2101 (15.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.78454\n",
      "INFO:tensorflow:loss = 0.68986154, step = 2201 (14.739 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.87133\n",
      "INFO:tensorflow:loss = 0.66651106, step = 2301 (14.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.81323\n",
      "INFO:tensorflow:loss = 0.714586, step = 2401 (14.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.35704\n",
      "INFO:tensorflow:loss = 0.6496476, step = 2501 (15.731 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.4304\n",
      "INFO:tensorflow:loss = 0.6886517, step = 2601 (15.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.78592\n",
      "INFO:tensorflow:loss = 0.7008257, step = 2701 (14.733 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.76845\n",
      "INFO:tensorflow:loss = 0.7140926, step = 2801 (14.775 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.76983\n",
      "INFO:tensorflow:loss = 0.70902926, step = 2901 (14.770 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.71027\n",
      "INFO:tensorflow:loss = 0.6792586, step = 3001 (14.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.71974\n",
      "INFO:tensorflow:loss = 0.69552386, step = 3101 (14.883 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.80163\n",
      "INFO:tensorflow:loss = 0.6293174, step = 3201 (14.702 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.68378\n",
      "INFO:tensorflow:loss = 0.70858896, step = 3301 (14.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.50959\n",
      "INFO:tensorflow:loss = 0.7380126, step = 3401 (15.362 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.83046\n",
      "INFO:tensorflow:loss = 0.7230983, step = 3501 (14.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.91078\n",
      "INFO:tensorflow:loss = 0.6500392, step = 3601 (14.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.89076\n",
      "INFO:tensorflow:loss = 0.7768274, step = 3701 (14.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.86001\n",
      "INFO:tensorflow:loss = 0.6891352, step = 3801 (14.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.86142\n",
      "INFO:tensorflow:loss = 0.7226827, step = 3901 (14.573 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.86755\n",
      "INFO:tensorflow:loss = 0.6485771, step = 4001 (14.562 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 4033...\n",
      "INFO:tensorflow:Saving checkpoints for 4033 into ./data_out/checkpoint/rnn\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 4033...\n",
      "INFO:tensorflow:global_step/sec: 6.38316\n",
      "INFO:tensorflow:loss = 0.63765824, step = 4101 (15.666 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.89647\n",
      "INFO:tensorflow:loss = 0.67305344, step = 4201 (14.500 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 4221...\n",
      "INFO:tensorflow:Saving checkpoints for 4221 into ./data_out/checkpoint/rnn\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 4221...\n",
      "INFO:tensorflow:Loss for final step: 0.60654277.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x24a1f342e80>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "# 모델 학습\n",
    "est.train(train_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-07-24T20:22:05Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./data_out/checkpoint/rnn\\model.ckpt-4221\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 3.57425s\n",
      "INFO:tensorflow:Finished evaluation at 2020-07-24-20:22:09\n",
      "INFO:tensorflow:Saving dict for global step 4221: acc = 0.5452, global_step = 4221, loss = 0.67958033\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4221: ./data_out/checkpoint/rnn\\model.ckpt-4221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.5452, 'loss': 0.67958033, 'global_step': 4221}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 평가\n",
    "est.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 캐글 평가 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './data_out/'\n",
    "TEST_INPUT_DATA = 'test_input.npy'\n",
    "TEST_ID_DATA = 'test_id.npy'\n",
    "\n",
    "test_input_data = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.estimator.inputs.numpy_input_fn함수를 활용해 데이터 입력 함수 생성\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\":test_input_data}, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jch\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_queue_runner.py:60: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\jch\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_functions.py:491: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./data_out/checkpoint/rnn\\model.ckpt-4221\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From C:\\Users\\jch\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:906: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    }
   ],
   "source": [
    "# estimator.predict로 모델을 통해 데이터를 예측한다.\n",
    "predictions = np.array([p['sentiment'] for p in est.predict(input_fn=\n",
    "predict_input_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-1d8a38882195>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_IN_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mTEST_ID_DATA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    450\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_memmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmmap_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[0;32m    453\u001b[0m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0;32m    454\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[1;31m# The array contained Python objects. We need to unpickle the data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m             raise ValueError(\"Object arrays cannot be loaded when \"\n\u001b[0m\u001b[0;32m    740\u001b[0m                              \"allow_pickle=False\")\n\u001b[0;32m    741\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "# 제출을 위해 평가 데이터의 id값을 불러온다.\n",
    "test_id = np.load(open(DATA_IN_PATH + TEST_ID_DATA, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "output = pd.DataFrame(data={\"id\": list(test_id), \"sentiment\":list(predictions)} )\n",
    "output.to_csv(DATA_OUT_PATH + 'movie_review_result_rnn.csv', index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
