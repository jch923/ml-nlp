{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 카글 텍스트 분류 - 합성곱 신경망 활용 접근방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jch\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# 기본적인 라이브러리들을 불러온다\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['vocab', 'vocab_size'])\n"
     ]
    }
   ],
   "source": [
    "# 이전에 저장했던 학습에 필요한 디렉터리 설정 및 학습/평가 데이터를 불러온다.\n",
    "DATA_IN_PATH = './data_in/'\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "TRAIN_INPUT_DATA = 'train_input.npy'\n",
    "TRAIN_LABEL_DATA = 'train_label.npy'\n",
    "TEST_INPUT_DATA = 'test_input.npy'\n",
    "TEST_ID_DATA = 'test_id.npy'\n",
    "\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "train_input_data = np.load(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'rb'))\n",
    "train_label_data = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'rb'))\n",
    "test_input_data = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, 'rb'))\n",
    "\n",
    "with open(DATA_IN_PATH + DATA_CONFIGS, 'r') as f:\n",
    "    prepro_configs = json.load(f)\n",
    "    print(prepro_configs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라메터 변수\n",
    "RNG_SEED = 1234\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "VOCAB_SIZE = prepro_configs['vocab_size'] + 1\n",
    "EMB_SIZE = 128\n",
    "VALID_SPLIT = 0.2\n",
    "\n",
    "# 학습 데이터와 평가 데이터로 나눈다.\n",
    "train_input, eval_input, train_label, eval_label = train_test_split(train_input_data, train_label_data, test_size=VALID_SPLIT, random_state=RNG_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매핑 함수\n",
    "def mapping_fn(X, Y=None):\n",
    "    input, label = {'x': X}, Y\n",
    "    return input, label\n",
    "\n",
    "# 학습 데이터 입력 함수\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_input, train_label))\n",
    "    dataset = dataset.shuffle(buffer_size=len(train_input))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    dataset = dataset.repeat(count=NUM_EPOCHS)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "# 평가데이터 입력 함수\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eval_input, eval_label))\n",
    "    dataset = dataset.shuffle(buffer_size=len(eval_input))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 대한 메인 부분입니다.\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    #embedding layer를 선언합니다.\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "                    VOCAB_SIZE,\n",
    "                    EMB_SIZE)(features['x'])\n",
    "    \n",
    "    # embedding layer에 대한 output에 대해 dropout을 취합니다.\n",
    "    dropout_emb = keras.layers.Dropout(rate=0.5)(embedding_layer)\n",
    "\n",
    "    ## filters = 128이고 kernel_size = 3,4,5입니다.\n",
    "    ## 길이가 3,4,5인 128개의 다른 필터를 생성합니다. 3,4,5 gram의 효과처럼 다양한 각도에서 문장을 보는 효과가 있습니다.\n",
    "    ## conv1d는 (배치사이즈, 길이, 채널)로 입력값을 받는데, 배치사이즈: 문장 숫자 | 길이: 각 문장의 단어의 개수 | 채널: 임베딩 출력 차원수임\n",
    "    ## 이후 각각의 합성곱 신경망에 맥스 풀링층을 적용한다.\n",
    "    ## 즉, 이 모델은 총 3개의 합성곱+맥스풀링층을 사용하는 구조다.\n",
    "    conv1 = keras.layers.Conv1D(\n",
    "         filters=128,\n",
    "         kernel_size=3,\n",
    "        padding='valid',\n",
    "         activation=tf.nn.relu)(dropout_emb)\n",
    "    \n",
    "    pool1 = keras.layers.GlobalMaxPool1D()(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(\n",
    "         filters=128,\n",
    "         kernel_size=4,\n",
    "        padding='valid',\n",
    "         activation=tf.nn.relu)(dropout_emb)\n",
    "    \n",
    "    pool2 = keras.layers.GlobalMaxPool1D()(conv2)\n",
    "    \n",
    "    conv3 = keras.layers.Conv1D(\n",
    "         filters=128,\n",
    "         kernel_size=5,\n",
    "        padding='valid',\n",
    "         activation=tf.nn.relu)(dropout_emb)\n",
    "    pool3 = keras.layers.GlobalMaxPool1D()(conv3)\n",
    "    \n",
    "    concat = keras.layers.concatenate([pool1, pool2, pool3]) #3,4,5gram이후 모아주기\n",
    "    \n",
    "    hidden = keras.layers.Dense(250, activation=tf.nn.relu)(concat)\n",
    "    dropout_hidden = keras.layers.Dropout(rate=0.5)(hidden)\n",
    "    logits = keras.layers.Dense(1, name='logits')(dropout_hidden)\n",
    "    logits = tf.squeeze(logits, axis=-1)\n",
    "    \n",
    "    #최종적으로 학습, 평가, 테스트의 단계로 나누어 활용\n",
    "    # 이전의 RNN과 똑같다.\n",
    "    \n",
    "    if PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions={\n",
    "                'prob': tf.nn.sigmoid(logits)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "\n",
    "    if EVAL:\n",
    "        pred = tf.nn.sigmoid(logits)\n",
    "        accuracy = tf.metrics.accuracy(labels, tf.round(pred))\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops={'acc': accuracy})\n",
    "        \n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\jch\\\\4.TEXT_CLASSIFICATION\\\\data_out/checkpoint/cnn/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 200, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 2, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 400, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From <ipython-input-10-982fae9b4d78>:14: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From C:\\Users\\jch\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:118: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 0.6940206, step = 1\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 200...\n",
      "INFO:tensorflow:Saving checkpoints for 200 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 200...\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 400...\n",
      "INFO:tensorflow:Saving checkpoints for 400 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "WARNING:tensorflow:From C:\\Users\\jch\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:969: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 400...\n",
      "INFO:tensorflow:global_step/sec: 9.32054\n",
      "INFO:tensorflow:loss = 0.7330744, step = 401 (42.917 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 600...\n",
      "INFO:tensorflow:Saving checkpoints for 600 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 600...\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 800...\n",
      "INFO:tensorflow:Saving checkpoints for 800 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 800...\n",
      "INFO:tensorflow:global_step/sec: 9.47703\n",
      "INFO:tensorflow:loss = 0.15186301, step = 801 (42.207 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1000...\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1000...\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1200...\n",
      "INFO:tensorflow:Saving checkpoints for 1200 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1200...\n",
      "INFO:tensorflow:global_step/sec: 9.26995\n",
      "INFO:tensorflow:loss = 0.55521345, step = 1201 (43.150 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1400...\n",
      "INFO:tensorflow:Saving checkpoints for 1400 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1400...\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1600...\n",
      "INFO:tensorflow:Saving checkpoints for 1600 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1600...\n",
      "INFO:tensorflow:global_step/sec: 9.21438\n",
      "INFO:tensorflow:loss = 0.02697801, step = 1601 (43.409 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1800...\n",
      "INFO:tensorflow:Saving checkpoints for 1800 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1800...\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2000...\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2000...\n",
      "INFO:tensorflow:global_step/sec: 9.33578\n",
      "INFO:tensorflow:loss = 0.006754906, step = 2001 (42.846 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2200...\n",
      "INFO:tensorflow:Saving checkpoints for 2200 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2200...\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2400...\n",
      "INFO:tensorflow:Saving checkpoints for 2400 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2400...\n",
      "INFO:tensorflow:global_step/sec: 9.47995\n",
      "INFO:tensorflow:loss = 0.013166209, step = 2401 (42.195 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2600...\n",
      "INFO:tensorflow:Saving checkpoints for 2600 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2600...\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2800...\n",
      "INFO:tensorflow:Saving checkpoints for 2800 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2800...\n",
      "INFO:tensorflow:global_step/sec: 9.45528\n",
      "INFO:tensorflow:loss = 0.0054306034, step = 2801 (42.303 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 3000...\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 3000...\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 3200...\n",
      "INFO:tensorflow:Saving checkpoints for 3200 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 3200...\n",
      "INFO:tensorflow:global_step/sec: 9.45819\n",
      "INFO:tensorflow:loss = 0.010695028, step = 3201 (42.291 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 3400...\n",
      "INFO:tensorflow:Saving checkpoints for 3400 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 3400...\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 3600...\n",
      "INFO:tensorflow:Saving checkpoints for 3600 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 3600...\n",
      "INFO:tensorflow:global_step/sec: 9.46783\n",
      "INFO:tensorflow:loss = 0.002160551, step = 3601 (42.248 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 3750...\n",
      "INFO:tensorflow:Saving checkpoints for 3750 into C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 3750...\n",
      "INFO:tensorflow:Loss for final step: 0.0005334609.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-07-26T17:31:41Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt-3750\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 1.78562s\n",
      "INFO:tensorflow:Finished evaluation at 2020-07-26-17:31:43\n",
      "INFO:tensorflow:Saving dict for global step 3750: acc = 0.878, global_step = 3750, loss = 0.4956636\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3750: C:\\Users\\jch\\4.TEXT_CLASSIFICATION\\data_out/checkpoint/cnn/model.ckpt-3750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.878, 'loss': 0.4956636, 'global_step': 3750}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = os.path.join(os.getcwd(), \"data_out/checkpoint/cnn/\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "config_tf = tf.estimator.RunConfig(save_checkpoints_steps=200, keep_checkpoint_max=2,\n",
    "                                    log_step_count_steps=400)\n",
    "\n",
    " #에스티메이터 객체 생성\n",
    "cnn_est = tf.estimator.Estimator(model_fn, model_dir=model_dir, config=config_tf)\n",
    "cnn_est.train(train_input_fn) #학습하기\n",
    "cnn_est.evaluate(eval_input_fn) #평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-2caa744c2fbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 캐글에 제출할 데이터 만들기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_input_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_IN_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mTEST_INPUT_DATA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_IN_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mTEST_ID_DATA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpredict_input_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy_input_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtest_input_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    450\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_memmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmmap_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[0;32m    453\u001b[0m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0;32m    454\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[1;31m# The array contained Python objects. We need to unpickle the data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m             raise ValueError(\"Object arrays cannot be loaded when \"\n\u001b[0m\u001b[0;32m    740\u001b[0m                              \"allow_pickle=False\")\n\u001b[0;32m    741\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "# 캐글에 제출할 데이터 만들기\n",
    "test_input_data = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, 'rb')) \n",
    "ids = np.load(open(DATA_IN_PATH + TEST_ID_DATA, 'rb'))\n",
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\":test_input_data}, shuffle=False)\n",
    "\n",
    "predictions = np.array([p['prob'] for p in cnn_est.predict(input_fn=predict_input_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame( data={\"id\": ids, \"sentiment\": predictions} )\n",
    "\n",
    "output.to_csv( DATA_OUT_PATH + \"Bag_of_Words_model_test.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
